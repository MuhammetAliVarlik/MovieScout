{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17964\\1258215059.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "d:\\MuhammetAliVarlik\\Anaconda\\envs\\MovieScout\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\"../data/processed/TMDB.csv\",encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Chroma database created and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17964\\224117220.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "persist_directory = \"../models/chroma_db\"\n",
    "\n",
    "# Check if the directory for the Chroma database exists\n",
    "if os.path.exists(persist_directory):\n",
    "    # Load the existing Chroma database\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
    "    print(\"Existing Chroma database loaded.\")\n",
    "else:\n",
    "    # If the database doesn't exist, create a new one\n",
    "    db = Chroma.from_documents(documents, embedding_function, persist_directory=persist_directory)\n",
    "    # Persist the new database\n",
    "    db.persist()\n",
    "    print(\"New Chroma database created and persisted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 6258\n",
      "title: Starship Troopers 2: Hero of the Federation\n",
      "vote_average: 4.279\n",
      "vote_count: 517\n",
      "status: Released\n",
      "release_date: 2004-04-24\n",
      "runtime: 92\n",
      "overview: In the sequel to Paul Verhoeven's loved/reviled sci-fi film, a group of troopers taking refuge in an abandoned outpost after fighting alien bugs, failing to realize that more danger lays in wait.\n",
      "poster_path: /9iqrqOjLaFcGNnSQlJxYLfgrhqn.jpg\n",
      "genres: Adventure, Horror, Action, Science Fiction\n",
      "backdrop_path: /kSEc72AYeZEgwPBHle8bGjmRnAC.jpg\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to watch a sci-fi movie.\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,PromptTemplate\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a movie recommendation system. Based on the context provided, recommend a movie:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Just give the answer in 1 or 2 sentences. Don't give spoilers.\n",
    "Answer (brief and direct):\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from ../models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.09 MiB\n",
      "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 1024\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    22.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   22.00 MiB, K (f16):   11.00 MiB, V (f16):   11.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    82.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'general.architecture': 'llama', 'llama.context_length': '2048', 'llama.rope.dimension_count': '64', 'llama.embedding_length': '2048', 'llama.block_count': '22', 'llama.feed_forward_length': '5632', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '4', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phi-3.5-mini-instruct-Q5_K_L.gguf\n",
    "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
    "Phi-3-mini-4k-instruct-q4.gguf\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Callback manager\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=\"../models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "    n_ctx=1024,  # Optimize context size based on your needs\n",
    "    verbose=True,  # Keep this only if necessary\n",
    "    use_mlock=True,  # Ensure enough memory is available\n",
    "    n_gpu_layers=16,  # Try increasing if you have enough GPU memory\n",
    "    n_threads=8,  # Try increasing this based on CPU cores\n",
    "    n_batch=1024,  # Increase batch size for higher throughput\n",
    "    callback_manager=callback_manager\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=[\n",
    "        'context', \n",
    "        'question',\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialise RetrievalQA Chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the movie recommendation system suggests a sci-fi movie that follows the journey of Princess Leia, the main character, and her attempts to save her people from the tyranny of the Empire. The film's title is \"Star Wars: Episodes II - Attack of the Cloones,\" and it stars Carrie Fisher as Princess Leia. It has a vote average of 8.204 out of 19,155 votes in its respective year, making it one of the highest-rated movies on the system's recommendation database."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13600.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   732 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19931.81 ms /   857 tokens\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"Recommend a sci-fi movies like Star Wars\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 726 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, \"Last Action Hero\" is a movie with a romantic thematic element. It should include scenes depicting characters falling in love and interacting with each other in a fun and lighthearted manner while also highlighting the action-packed aspects of the film's plot. Additionally, it should have elements of humor and adventure that bring the characters together as they fight against villainous antagonists. The movie's runtime is 131 minutes, with a vote average of 6.432 out of 7.03, a rating of \"Relaysed,\" and an average review score of 8.3/10 from Rotten Tomatoes."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13600.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   726 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   21350.45 ms /   874 tokens\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"Recommend an commedy movie. But it should include rommence theme also\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 629 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, \"Friends with Benefits\" is a movie recommended by the author for its theme of friendship. The movie involves two individuals, Dylan and Jamie, who are good friends, but decide to explore their sexual desires after one night of unprotected sex. Although the movie does have elements of a Hollywood romantic comedy, it explores the complexities of friendship as well as the challenges that come with trying something new. The title is a play on the popular phrase \"friends for life,\" which suggests a deep connection between two individuals regardless of whether or not they have been together long-term. Overall, this movie provides a valuable insight into the power and importance of friendship in a wide range of situations, from romantic encounters to personal relationships."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13600.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   629 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   164 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20774.15 ms /   793 tokens\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"Recommend a movie with a theme of friendship\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 700 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a superhero movie with a female main star, the context provided suggests \"Supergirl,\" which stars the titular character as an alien cousin of Superman. While Karla (Kara), as she is known in the story, faces a powerful enemy wicked witch to retrieve an orb lost by her cousin and find herself up against the witch, the movie's overview includes being set on Earth during a galactic war between two alien races. The release dates of the movie also suggest it is scheduled for 2019-2023 and the movie features the Justice League x RWBY: Super Heroes & Huntsmen Part One, with the release date indicating the movie will air in April 2023 on various streaming platforms. Overall, based on this information, \"Supergirl\" would likely fit the requirements of a superhero movie with a female main star."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13600.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   700 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   23155.90 ms /   893 tokens\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"I want to watch a superhero movie with a female main star\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 699 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To recommend a movie that is guaranteed to make you cry, I would suggest the classic rom-com \"The Notebook\" (2004). This film follows two teenage lovers who navigate the trials of life in a small Southern town during the Great Depression. The emotional intensity of this film comes from its beautiful and poignant portrayal of love and loss, making it an ideal choice for fans of heartwarming tales with heartbreaking endings."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13600.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   699 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17481.74 ms /   800 tokens\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"I want you to recommend movies that are guaranteed to make you cry.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MovieScout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
